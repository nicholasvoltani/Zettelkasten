---
Date: Wednesday, 02-06-2021 @ 18:23
Source:
Tags: #information_theory

---
# Definição: Entropia
Dada uma variável aleatória $X$ com distribuição de probabilidade $p$, temos que sua entropia é definida como
$$H(X) \equiv - \sum\limits_{x} p(x) \log p(x)$$ 

Devido à definição de [[Def - Informação Mútua| informação mútua]], podemos dizer que a entropia é uma medida de *autoinformação* da variável $X$.

---
### References
- [[]]